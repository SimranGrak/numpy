import pandas as pd


#Dataframes= 2D labelled array
info={
    "name": ["robin", "simran", "Rian"],
    "CGPA": [9.8, 8.8, 9.0]
}

df=pd.DataFrame(info)
print(df)




df


#Series: 1D labelled array
s=pd.Series([1,2,3,4,5])

print(s)

#Two types  of accessing: Indexing and lables
#Indexing
print(s[2])
print(s[4])


#labels: can assign custom labels
age=pd.Series([22, 23, 24, 25, 26], index=["bob", "charlie", "adam", "chloe", "ashley"])

print(age)

#accessing through labels
print(age["adam"])
print(age["bob"])
print(age.index)


#series properties
s1=pd.Series([1, 2, 3, 4, 5])
s2=pd.Series([10, 20, 30, 40, 50])

print(s1+s2)

s1[3]=100

print(s1)


changed_s1=s1.drop(2)

print(changed_s1)

print(s1)


#Dataframe in detail
info={
    "name":["robin", "simran", "rian"],
    "age":[22,22,1],
    "GPA":[9.0, 9.8, 9.9]    
}

s=pd.DataFrame(info)
print(s)


#accessing rows labels
print(s.index)
print(s.columns)


#creating dataframe with list of list

s=pd.DataFrame([["robin", 22],["simran", 23],["rian", 1]], columns=["name", "age"])

print(s)


#creating dataframe array from numpy array
import numpy as np

arr=np.array([[1,2,3],[4,5,6],[7,8,9]])

df=pd.DataFrame(arr, columns=["A", "B", "C"])

print(df)


#CSV data
df=pd.read_csv("employee_data.csv")

print(df, type(df))

#json data
df1=pd.read_json("employee_data.json", orient="columns")

print(df1, type(df1))


#Dataframes methods

df

df.head()                                          #prints first five values
df.tail()                                          #print last five values
df.tail(2)
df.sample(2)                                        #runs randomly any values
df.info()                                           #summary of our data
df.shape                                          #returns tuples of rows and columns
df.describe()                                       #returns the basic information of the data
df.columns                   
df.nunique()                                        #returns n number of unique rows


#Global Air Quality Data
data=pd.read_csv("globalAirQuality.csv")
print(data)




data

print(data.head(5))
print(data.tail())
print(data.info())
print(data.shape)
print(data.describe())
print(data.columns)
print(data.nunique())
print(data.sample())


#selecting data
print(data["city"])
print(data[["city", "aqi", "co"]])

print(data.loc[0])                         #accessing row using labels
print(data.iloc[2])                        #accessing row using indexes
#slicing
print(data.loc[2:7])


#getting rows and columns both values
print(data.loc[0, "aqi"])

#or we can multiple values
print(data.loc[1, ["aqi", "temperature"]])
#cannot execute through iloc beacuse it needs the numerical value

#select single scaler value
print(data.at[0, "city"])
print(data.iat[0,5])


#similar as numpy, pandas create view not the copy and if we want to use copy we have to use .copy()

data



#Filtering data
#Question 1: WAP to get the aqi above than 100

print(data[ data["aqi"] > 100])


#Question 2: WAP to get the aqi above than 100 and temperature above than 30

print(data[ (data["aqi"] >100) & (data["temperature"] > 30)])

print(data[ data["aqi"]> 100] [["city", "aqi"]])


#query

print(data.query("aqi> 100" )[["city", "country"]])


#in query we can access variables: returns copy
aqi_val=100

print(data.query("aqi> @aqi_val")[["city", "country"]])


#Data cleaning

#1. Handle missing values
raw_data=pd.read_csv("raw_data.csv")
raw_data.isnull()

#WAP to find the sum of missing values in every column
raw_data.isnull().sum()

#drop the row whose value is missing
raw_data.dropna()

#drop the column whose value is missing
raw_data.dropna(axis=1)


#fill the empty value
raw_data.fillna(0)


#fill the empty value of particular column
age_mean=raw_data["age"].mean()

cleaned_data=raw_data.copy()
age_mean=cleaned_data["age"].mean()

cleaned_data["age"]=cleaned_data["age"].fillna(age_mean)



#forward fill
raw_data.ffill()

#backward fill
raw_data.bfill()


raw_data


#handling duplicates
raw_data.duplicated()

#drop duplicate
cleaned_data=raw_data.drop_duplicates().copy()               #it will not reflect in the original data

cleaned_data


#data types

#check data type
raw_data.dtypes

cleaned_data=raw_data.copy()
cleaned_data=raw_data.fillna(0)

#change the data type
cleaned_data["age"]=cleaned_data["age"].astype("int64")

cleaned_data.dtypes

#changing type into date time data type
df=pd.Series([pd.to_datetime("12-10-31")])

type(df.dtypes)





data=pd.read_csv("globalAirQuality.csv")

data["timestamp"]=pd.to_datetime(data["timestamp"])

data["timestamp"].dtype


raw_data["gender"].str.lower()
raw_data["gender"].str.upper()

#separate the values
raw_data["name"].str.split(" ")            #on the basis of space and also can do on the basis of anything like @, any letter


#check if any value is present or not
raw_data["country"].str.contains("USA")

#we can check after ignoring cases also
raw_data["country"].str.contains("india", case=False)


#Feature engineering

#apply method

data2=raw_data.copy()

data2["tax"]=raw_data["income"].apply(lambda x: "20%" if x>50000 else "10%")




#map method: to replace existing value to new value
gender_map={"Female": "F", "Male": "M", "Unknown": "U"}

df2=raw_data.copy()

df2["gender"]=df2["gender"].map(gender_map)



#assigning methods: creating new columns

df2=df2.assign(new_income=df2["income"]*1.1)




#replace method

df2["country"]=df2["country"].replace("USA", "US")




#renaming all columns

df2.columns=["ID", "NAME", "AGE", "COUNTRY", "GENDER", "INCOME", "NEW_INCOME"]



#rename particular column

df2.rename(columns={"INCOME": "SALARY"})

#change particular row using labels

df2.rename(index={1:"First"})

#sort values according to columns

sorted_data=df2.sort_values("INCOME")


# sorted_data=df2.sort_values("INCOME", ascending=False)

# print(sorted_data)


#sort values acccording to labels

sorted_labels=df2.sort_index

# print(sorted_labels)


#reset
# sorted_data=sorted_data.reset_index()

#drop the auto saving labels

sorted_data=sorted_data.reset_index(drop= True)

# print(sorted_data)


#ranking

sorted_data["RANKING"]= sorted_data["INCOME"].rank(ascending=False, method="max")

# print(sorted_data)


#reorder columns
sorted_data[["ID", "NAME", "AGE", "COUNTRY", "GENDER", "INCOME","RANKING", "NEW_INCOME"]]



raw_data



#Practice question

#Question: Shift id column to end

shift_data=raw_data[["name", "age", "country", "gender", "income", "id"]]

shift_data


#or we can do with list comprehension
new_col_order=raw_data.copy()

new_col_order=[col for col in raw_data.columns if col!="id"] +["id"]

print(new_col_order)

raw_data[new_col_order]


#writing data to json and csv file
cleaned_data=raw_data.copy()

cleaned_data=cleaned_data.drop_duplicates()
cleaned_data=cleaned_data.fillna(0)
cleaned_data=cleaned_data.sort_values("income")
cleaned_data=cleaned_data.reset_index(drop=True)


cleaned_data.to_csv("cleaned_data.csv")

cleaned_data



#Grouping and aggregation of Data
raw_data.groupby("country")["income"].min()
raw_data.groupby("country")["income"].max()
raw_data.groupby("gender")["income"].mean()


#aggregate: means use multiple operation at one time
raw_data.groupby("country")["income"].agg(["mean","min","max"])

#we can rename the aggregate values
raw_data.groupby("country")["income"].aggregate(avg_salary="mean", min_salary="min", max_salary="max")

#we can apply multiple operations on multiple values
raw_data.groupby("country").agg(
    avg_salary=("income","mean"),
    max_age=("age","max")
)



#Melt and Pivot function

#melt function: wide to long format
melted_data=raw_data.melt(
    id_vars=["id","country", "name"],
    value_vars=["age","gender", "income"],
    var_name="metrics",
    value_name="value"
)

melted_data


#pivot function: long format to wide format
# original_data=melted_data.pivot(
#     index=["id","country", "name"],
#     columns="metrics",
#     values="value"
# )

# original_data


raw_data


#Basic Visualization in Pandas
df= pd.read_csv("employee_data.csv")

#hist
df["Age"].hist()

#plot
df.plot(kind="scatter", x="Age", y=")


raw_data




